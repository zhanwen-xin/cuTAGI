{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-learning using backprop on small grdiworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "from src.gym_kalman.env_Gridworld import GridworldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize value function\n",
    "import numpy as np\n",
    "\n",
    "# initialize the environment\n",
    "grid_size = 4\n",
    "env = GridworldEnv(grid_size=grid_size, reward_std=0.)\n",
    "num_states = env.observation_space.n\n",
    "actions = np.arange(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_encoder(state):\n",
    "    # one hot encoding\n",
    "    state_onehot = np.zeros(num_states)\n",
    "    state_onehot[state] = 1\n",
    "    return state_onehot\n",
    "\n",
    "def state_decoder(state_onehot):\n",
    "    return np.argmax(state_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 1\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.0001\n",
    "EPS_DECAY = 1000\n",
    "\n",
    "TAU = 0.005\n",
    "LR = 1e-2\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = 16\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "def select_greedy_action(state):\n",
    "    with torch.no_grad():\n",
    "        return policy_net(state).max(1).indices.view(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract current policy\n",
    "def extract_policy(num_states, episode_i):\n",
    "    policy = np.zeros(num_states)\n",
    "    for state in range(num_states):\n",
    "        if state == 15:  # Terminal state\n",
    "            policy[state] = 10\n",
    "            continue\n",
    "        state_encoded = state_encoder(state)\n",
    "        state_tensor = torch.tensor(state_encoded, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        suggested_action = select_greedy_action(state_tensor)\n",
    "        policy[state] = suggested_action\n",
    "\n",
    "    policy_grid = np.array(policy).reshape((grid_size, grid_size))\n",
    "    # Print title of the plot\n",
    "    print(f\"Episode {episode_i}'s policy\")\n",
    "    print(policy_grid)\n",
    "    return\n",
    "\n",
    "def print_state_values(num_states, i_episode):\n",
    "    ########################## Print the state values ##########################\n",
    "    print(f\"\\nEpisode {i_episode}'s state values\")\n",
    "    values = np.zeros(num_states)\n",
    "    for state in range(num_states):\n",
    "        if state == 15:  # Terminal state\n",
    "            continue\n",
    "        state_encoded = state_encoder(state)\n",
    "        state_tensor = torch.tensor(state_encoded, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        suggested_action = select_greedy_action(state_tensor)\n",
    "        with torch.no_grad():\n",
    "            values[state] = policy_net(state_tensor)[0].tolist()[suggested_action]\n",
    "\n",
    "    value_grid = np.array(values).reshape((grid_size, grid_size))\n",
    "    value_grid = np.round(value_grid, 2)\n",
    "    print(value_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 0's state values\n",
      "[[0.09 0.1  0.09 0.1 ]\n",
      " [0.11 0.17 0.11 0.12]\n",
      " [0.1  0.07 0.13 0.11]\n",
      " [0.12 0.09 0.12 0.  ]]\n",
      "\n",
      "Episode 1's state values\n",
      "[[0.09 0.1  0.09 0.1 ]\n",
      " [0.11 0.17 0.11 0.12]\n",
      " [0.1  0.07 0.13 0.11]\n",
      " [0.12 0.09 0.12 0.  ]]\n",
      "\n",
      "Episode 2's state values\n",
      "[[-0.92 -0.89 -0.94 -0.92]\n",
      " [-0.91 -0.82 -0.89 -0.91]\n",
      " [-0.93 -0.87 -0.92 -0.88]\n",
      " [-0.94 -0.92 -0.96  0.  ]]\n",
      "\n",
      "Episode 3's state values\n",
      "[[-1.23 -1.19 -1.22 -1.19]\n",
      " [-1.2  -1.14 -1.19 -1.19]\n",
      " [-1.22 -1.19 -1.19 -1.09]\n",
      " [-1.2  -1.17 -1.15  0.  ]]\n",
      "\n",
      "Episode 4's state values\n",
      "[[-1.33 -1.29 -1.32 -1.29]\n",
      " [-1.3  -1.27 -1.3  -1.29]\n",
      " [-1.32 -1.3  -1.27 -1.02]\n",
      " [-1.29 -1.26 -1.01  0.  ]]\n",
      "\n",
      "Episode 5's state values\n",
      "[[-1.39 -1.37 -1.38 -1.36]\n",
      " [-1.37 -1.35 -1.35 -1.35]\n",
      " [-1.38 -1.37 -1.33 -1.01]\n",
      " [-1.36 -1.31 -0.98  0.  ]]\n",
      "\n",
      "Episode 6's state values\n",
      "[[-1.43 -1.41 -1.43 -1.4 ]\n",
      " [-1.42 -1.4  -1.41 -1.41]\n",
      " [-1.43 -1.42 -1.37 -1.02]\n",
      " [-1.42 -1.37 -0.97  0.  ]]\n",
      "\n",
      "Episode 7's state values\n",
      "[[-1.71 -1.69 -1.7  -1.68]\n",
      " [-1.71 -1.71 -1.69 -1.62]\n",
      " [-1.72 -1.69 -1.56 -1.01]\n",
      " [-1.68 -1.53 -1.14  0.  ]]\n",
      "\n",
      "Episode 8's state values\n",
      "[[-1.88 -1.87 -1.86 -1.85]\n",
      " [-1.85 -1.85 -1.84 -1.69]\n",
      " [-1.85 -1.85 -1.66 -1.01]\n",
      " [-1.84 -1.66 -0.99  0.  ]]\n",
      "\n",
      "Episode 9's state values\n",
      "[[-1.91 -1.89 -1.89 -1.87]\n",
      " [-1.89 -1.89 -1.88 -1.7 ]\n",
      " [-1.89 -1.87 -1.67 -1.01]\n",
      " [-1.87 -1.67 -1.01  0.  ]]\n",
      "\n",
      "Episode 10's state values\n",
      "[[-1.95 -1.92 -1.93 -1.91]\n",
      " [-1.92 -1.92 -1.91 -1.71]\n",
      " [-1.92 -1.9  -1.68 -1.  ]\n",
      " [-1.9  -1.68 -1.02  0.  ]]\n",
      "\n",
      "Episode 11's state values\n",
      "[[-2.05 -2.03 -2.03 -2.02]\n",
      " [-2.03 -2.02 -2.01 -1.76]\n",
      " [-2.02 -1.98 -1.72 -1.  ]\n",
      " [-1.98 -1.72 -1.    0.  ]]\n",
      "\n",
      "Episode 12's state values\n",
      "[[-2.08 -2.05 -2.06 -2.04]\n",
      " [-2.05 -2.04 -2.03 -1.77]\n",
      " [-2.04 -1.99 -1.72 -1.  ]\n",
      " [-2.   -1.72 -1.01  0.  ]]\n",
      "\n",
      "Episode 13's state values\n",
      "[[-2.14 -2.11 -2.12 -2.09]\n",
      " [-2.11 -2.1  -2.08 -1.78]\n",
      " [-2.1  -2.04 -1.74 -1.01]\n",
      " [-2.04 -1.74 -1.01  0.  ]]\n",
      "\n",
      "Episode 14's state values\n",
      "[[-2.2  -2.17 -2.17 -2.13]\n",
      " [-2.17 -2.15 -2.12 -1.8 ]\n",
      " [-2.16 -2.08 -1.76 -1.  ]\n",
      " [-2.08 -1.76 -1.    0.  ]]\n",
      "\n",
      "Episode 15's state values\n",
      "[[-2.25 -2.22 -2.23 -2.17]\n",
      " [-2.22 -2.21 -2.16 -1.82]\n",
      " [-2.21 -2.12 -1.77 -1.  ]\n",
      " [-2.12 -1.78 -1.    0.  ]]\n",
      "\n",
      "Episode 16's state values\n",
      "[[-2.3  -2.27 -2.28 -2.21]\n",
      " [-2.27 -2.25 -2.19 -1.83]\n",
      " [-2.25 -2.15 -1.78 -1.  ]\n",
      " [-2.16 -1.78 -1.    0.  ]]\n",
      "\n",
      "Episode 17's state values\n",
      "[[-2.33 -2.3  -2.31 -2.23]\n",
      " [-2.3  -2.29 -2.21 -1.84]\n",
      " [-2.29 -2.18 -1.79 -1.  ]\n",
      " [-2.18 -1.79 -1.    0.  ]]\n",
      "\n",
      "Episode 18's state values\n",
      "[[-2.38 -2.35 -2.36 -2.26]\n",
      " [-2.35 -2.33 -2.24 -1.85]\n",
      " [-2.33 -2.21 -1.8  -1.  ]\n",
      " [-2.21 -1.81 -1.    0.  ]]\n",
      "\n",
      "Episode 19's state values\n",
      "[[-2.48 -2.45 -2.46 -2.33]\n",
      " [-2.46 -2.43 -2.3  -1.87]\n",
      " [-2.43 -2.29 -1.81 -1.  ]\n",
      " [-2.28 -1.83 -1.01  0.  ]]\n",
      "\n",
      "Episode 20's state values\n",
      "[[-2.52 -2.5  -2.5  -2.36]\n",
      " [-2.5  -2.47 -2.33 -1.88]\n",
      " [-2.47 -2.3  -1.84 -1.  ]\n",
      " [-2.31 -1.84 -1.    0.  ]]\n",
      "\n",
      "Episode 21's state values\n",
      "[[-2.59 -2.56 -2.56 -2.39]\n",
      " [-2.57 -2.52 -2.36 -1.89]\n",
      " [-2.52 -2.35 -1.84 -0.99]\n",
      " [-2.35 -1.85 -1.01  0.  ]]\n",
      "\n",
      "Episode 22's state values\n",
      "[[-2.66 -2.62 -2.61 -2.43]\n",
      " [-2.62 -2.58 -2.41 -1.91]\n",
      " [-2.58 -2.38 -1.87 -1.  ]\n",
      " [-2.38 -1.9  -1.    0.  ]]\n",
      "\n",
      "Episode 23's state values\n",
      "[[-2.67 -2.67 -2.63 -2.47]\n",
      " [-2.64 -2.61 -2.43 -1.92]\n",
      " [-2.59 -2.41 -1.88 -1.02]\n",
      " [-2.39 -1.89 -1.    0.  ]]\n",
      "\n",
      "Episode 24's state values\n",
      "[[-2.72 -2.71 -2.67 -2.52]\n",
      " [-2.7  -2.65 -2.47 -1.93]\n",
      " [-2.64 -2.42 -1.89 -1.02]\n",
      " [-2.44 -1.88 -1.    0.  ]]\n",
      "\n",
      "Episode 25's state values\n",
      "[[-2.75 -2.72 -2.68 -2.48]\n",
      " [-2.72 -2.65 -2.47 -1.93]\n",
      " [-2.63 -2.44 -1.9  -1.01]\n",
      " [-2.43 -1.86 -1.    0.  ]]\n",
      "\n",
      "Episode 26's state values\n",
      "[[-2.78 -2.74 -2.72 -2.48]\n",
      " [-2.75 -2.67 -2.48 -1.92]\n",
      " [-2.7  -2.44 -1.86 -1.01]\n",
      " [-2.48 -1.86 -1.    0.  ]]\n",
      "\n",
      "Episode 27's state values\n",
      "[[-2.86 -2.8  -2.77 -2.52]\n",
      " [-2.83 -2.72 -2.53 -1.94]\n",
      " [-2.76 -2.47 -1.91 -1.  ]\n",
      " [-2.48 -1.87 -1.    0.  ]]\n",
      "\n",
      "Episode 28's state values\n",
      "[[-2.82 -2.74 -2.73 -2.47]\n",
      " [-2.8  -2.68 -2.47 -1.85]\n",
      " [-2.69 -2.4  -1.8  -0.94]\n",
      " [-2.49 -1.9  -0.96  0.  ]]\n",
      "\n",
      "Episode 29's state values\n",
      "[[-2.98 -2.89 -2.84 -2.6 ]\n",
      " [-2.94 -2.82 -2.49 -1.93]\n",
      " [-2.79 -2.58 -1.91 -1.  ]\n",
      " [-2.63 -1.92 -1.04  0.  ]]\n",
      "\n",
      "Episode 30's state values\n",
      "[[-3.   -2.94 -2.92 -2.6 ]\n",
      " [-3.02 -2.84 -2.54 -1.92]\n",
      " [-2.82 -2.54 -1.9  -0.95]\n",
      " [-2.62 -1.95 -0.98  0.  ]]\n",
      "\n",
      "Episode 31's state values\n",
      "[[-3.06 -3.04 -2.95 -2.62]\n",
      " [-3.05 -2.87 -2.58 -1.96]\n",
      " [-2.92 -2.61 -1.92 -0.99]\n",
      " [-2.51 -1.92 -1.    0.  ]]\n",
      "\n",
      "Episode 32's state values\n",
      "[[-3.09 -3.06 -2.95 -2.62]\n",
      " [-3.05 -2.97 -2.59 -1.94]\n",
      " [-2.86 -2.62 -1.92 -1.  ]\n",
      " [-2.58 -1.87 -1.    0.  ]]\n",
      "\n",
      "Episode 33's state values\n",
      "[[-3.18 -3.18 -3.05 -2.68]\n",
      " [-3.14 -2.98 -2.66 -1.97]\n",
      " [-2.96 -2.61 -1.93 -1.02]\n",
      " [-2.68 -1.97 -1.02  0.  ]]\n",
      "\n",
      "Episode 34's state values\n",
      "[[-3.18 -3.14 -3.01 -2.64]\n",
      " [-3.1  -2.98 -2.63 -1.93]\n",
      " [-2.9  -2.57 -1.89 -0.98]\n",
      " [-2.59 -1.84 -0.94  0.  ]]\n",
      "\n",
      "Episode 35's state values\n",
      "[[-3.34 -3.27 -3.14 -2.74]\n",
      " [-3.26 -3.14 -2.71 -1.98]\n",
      " [-3.1  -2.66 -2.   -1.04]\n",
      " [-2.68 -1.95 -1.04  0.  ]]\n",
      "\n",
      "Episode 36's state values\n",
      "[[-3.27 -3.23 -3.09 -2.68]\n",
      " [-3.24 -3.03 -2.69 -1.92]\n",
      " [-3.08 -2.61 -1.94 -0.99]\n",
      " [-2.66 -1.97 -1.01  0.  ]]\n",
      "\n",
      "Episode 37's state values\n",
      "[[-3.37 -3.29 -3.15 -2.72]\n",
      " [-3.33 -3.09 -2.69 -1.99]\n",
      " [-3.14 -2.66 -1.95 -1.01]\n",
      " [-2.7  -1.94 -1.    0.  ]]\n",
      "\n",
      "Episode 38's state values\n",
      "[[-3.37 -3.32 -3.15 -2.72]\n",
      " [-3.29 -3.12 -2.7  -1.95]\n",
      " [-3.06 -2.68 -1.95 -0.99]\n",
      " [-2.65 -1.93 -1.01  0.  ]]\n",
      "\n",
      "Episode 39's state values\n",
      "[[-3.4  -3.31 -3.16 -2.72]\n",
      " [-3.31 -3.11 -2.68 -1.96]\n",
      " [-3.15 -2.67 -1.94 -1.  ]\n",
      " [-2.71 -1.98 -1.    0.  ]]\n",
      "\n",
      "Episode 40's state values\n",
      "[[-3.43 -3.38 -3.2  -2.75]\n",
      " [-3.36 -3.15 -2.7  -1.96]\n",
      " [-3.14 -2.7  -1.94 -0.98]\n",
      " [-2.67 -1.95 -0.99  0.  ]]\n",
      "\n",
      "Episode 41's state values\n",
      "[[-3.49 -3.43 -3.24 -2.79]\n",
      " [-3.4  -3.19 -2.74 -2.  ]\n",
      " [-3.15 -2.74 -1.96 -1.  ]\n",
      " [-2.69 -1.92 -0.99  0.  ]]\n",
      "\n",
      "Episode 42's state values\n",
      "[[-3.55 -3.45 -3.25 -2.79]\n",
      " [-3.43 -3.23 -2.77 -1.99]\n",
      " [-3.2  -2.75 -1.98 -1.03]\n",
      " [-2.71 -1.96 -1.    0.  ]]\n",
      "\n",
      "Episode 43's state values\n",
      "[[-3.6  -3.48 -3.23 -2.79]\n",
      " [-3.55 -3.2  -2.79 -2.01]\n",
      " [-3.26 -2.75 -1.95 -1.03]\n",
      " [-2.76 -1.97 -1.03  0.  ]]\n",
      "\n",
      "Episode 44's state values\n",
      "[[-3.55 -3.44 -3.17 -2.75]\n",
      " [-3.46 -3.19 -2.71 -1.94]\n",
      " [-3.15 -2.69 -1.9  -0.96]\n",
      " [-2.65 -1.86 -0.92  0.  ]]\n",
      "\n",
      "Episode 45's state values\n",
      "[[-3.56 -3.47 -3.25 -2.67]\n",
      " [-3.4  -3.17 -2.66 -1.87]\n",
      " [-3.19 -2.72 -2.   -0.95]\n",
      " [-2.73 -1.84 -0.94  0.  ]]\n",
      "\n",
      "Episode 46's state values\n",
      "[[-3.66 -3.49 -3.17 -2.73]\n",
      " [-3.53 -3.13 -2.68 -1.87]\n",
      " [-3.13 -2.71 -1.7  -0.96]\n",
      " [-2.64 -1.85 -0.96  0.  ]]\n",
      "\n",
      "Episode 47's state values\n",
      "[[-3.83 -3.71 -3.39 -2.83]\n",
      " [-3.67 -3.37 -2.79 -2.01]\n",
      " [-3.49 -2.78 -2.02 -0.97]\n",
      " [-2.81 -1.93 -0.93  0.  ]]\n",
      "\n",
      "Episode 48's state values\n",
      "[[-3.81 -3.69 -3.42 -2.83]\n",
      " [-3.67 -3.36 -2.78 -1.94]\n",
      " [-3.34 -2.72 -1.95 -1.03]\n",
      " [-2.76 -1.94 -1.03  0.  ]]\n",
      "\n",
      "Episode 49's state values\n",
      "[[-3.86 -3.7  -3.41 -2.84]\n",
      " [-3.71 -3.36 -2.81 -1.96]\n",
      " [-3.39 -2.8  -1.95 -0.99]\n",
      " [-2.86 -1.98 -1.    0.  ]]\n",
      "\n",
      "Episode 50's state values\n",
      "[[-3.95 -3.77 -3.47 -2.85]\n",
      " [-3.76 -3.44 -2.81 -1.98]\n",
      " [-3.41 -2.77 -1.94 -0.94]\n",
      " [-2.81 -1.94 -1.    0.  ]]\n",
      "\n",
      "Episode 51's state values\n",
      "[[-3.91 -3.74 -3.42 -2.8 ]\n",
      " [-3.72 -3.39 -2.8  -1.95]\n",
      " [-3.38 -2.8  -1.96 -1.01]\n",
      " [-2.86 -1.98 -0.99  0.  ]]\n",
      "\n",
      "Episode 52's state values\n",
      "[[-4.04 -3.84 -3.51 -2.89]\n",
      " [-3.87 -3.44 -2.9  -1.97]\n",
      " [-3.48 -2.84 -2.   -1.08]\n",
      " [-2.85 -1.98 -1.03  0.  ]]\n",
      "\n",
      "Episode 53's state values\n",
      "[[-4.06 -3.89 -3.51 -2.9 ]\n",
      " [-3.83 -3.53 -2.87 -2.04]\n",
      " [-3.48 -2.86 -1.99 -0.99]\n",
      " [-2.83 -1.99 -1.02  0.  ]]\n",
      "\n",
      "Episode 54's state values\n",
      "[[-4.02 -3.82 -3.48 -2.86]\n",
      " [-3.82 -3.47 -2.78 -1.96]\n",
      " [-3.43 -2.81 -1.94 -1.  ]\n",
      " [-2.8  -1.93 -0.97  0.  ]]\n",
      "\n",
      "Episode 55's state values\n",
      "[[-4.12 -3.88 -3.52 -2.87]\n",
      " [-3.87 -3.52 -2.82 -1.95]\n",
      " [-3.45 -2.8  -1.95 -0.98]\n",
      " [-2.82 -1.96 -0.99  0.  ]]\n",
      "\n",
      "Episode 56's state values\n",
      "[[-4.13 -3.93 -3.54 -2.89]\n",
      " [-3.89 -3.51 -2.84 -1.97]\n",
      " [-3.49 -2.81 -1.97 -1.01]\n",
      " [-2.8  -1.94 -0.98  0.  ]]\n",
      "\n",
      "Episode 57's state values\n",
      "[[-4.2  -4.01 -3.62 -2.93]\n",
      " [-3.96 -3.56 -2.88 -2.01]\n",
      " [-3.56 -2.89 -2.01 -1.05]\n",
      " [-2.88 -2.01 -1.04  0.  ]]\n",
      "\n",
      "Episode 58's state values\n",
      "[[-4.21 -4.   -3.56 -2.93]\n",
      " [-3.99 -3.56 -2.88 -1.98]\n",
      " [-3.54 -2.86 -1.95 -0.97]\n",
      " [-2.87 -1.97 -1.01  0.  ]]\n",
      "\n",
      "Episode 59's state values\n",
      "[[-4.25 -4.04 -3.6  -2.93]\n",
      " [-3.98 -3.58 -2.92 -2.03]\n",
      " [-3.53 -2.9  -1.99 -1.05]\n",
      " [-2.85 -1.99 -1.02  0.  ]]\n",
      "\n",
      "Episode 60's state values\n",
      "[[-4.26 -3.99 -3.58 -2.84]\n",
      " [-4.01 -3.54 -2.81 -1.96]\n",
      " [-3.54 -2.85 -1.97 -0.94]\n",
      " [-2.86 -1.98 -1.    0.  ]]\n",
      "\n",
      "Episode 61's state values\n",
      "[[-4.32 -4.04 -3.6  -2.89]\n",
      " [-4.02 -3.57 -2.88 -1.97]\n",
      " [-3.56 -2.84 -1.97 -1.09]\n",
      " [-2.86 -1.95 -0.99  0.  ]]\n",
      "\n",
      "Episode 62's state values\n",
      "[[-4.34 -4.09 -3.63 -2.9 ]\n",
      " [-4.06 -3.6  -2.9  -2.01]\n",
      " [-3.6  -2.88 -1.97 -0.97]\n",
      " [-2.88 -1.98 -1.    0.  ]]\n",
      "\n",
      "Episode 63's state values\n",
      "[[-4.39 -4.13 -3.65 -2.95]\n",
      " [-4.11 -3.64 -2.91 -2.  ]\n",
      " [-3.64 -2.91 -2.   -1.05]\n",
      " [-2.89 -1.99 -1.02  0.  ]]\n",
      "\n",
      "Episode 64's state values\n",
      "[[-4.35 -4.08 -3.65 -2.88]\n",
      " [-4.12 -3.58 -2.89 -1.97]\n",
      " [-3.57 -2.86 -1.97 -0.96]\n",
      " [-2.86 -1.96 -1.    0.  ]]\n",
      "\n",
      "Episode 65's state values\n",
      "[[-4.41 -4.17 -3.69 -2.91]\n",
      " [-4.2  -3.62 -2.89 -2.01]\n",
      " [-3.61 -2.86 -1.99 -0.93]\n",
      " [-2.92 -1.99 -1.    0.  ]]\n",
      "\n",
      "Episode 66's state values\n",
      "[[-4.5  -4.23 -3.75 -3.02]\n",
      " [-4.22 -3.71 -2.94 -2.03]\n",
      " [-3.71 -2.97 -2.04 -1.01]\n",
      " [-2.94 -2.06 -1.07  0.  ]]\n",
      "\n",
      "Episode 67's state values\n",
      "[[-4.45 -4.18 -3.64 -2.89]\n",
      " [-4.15 -3.68 -2.89 -1.92]\n",
      " [-3.67 -2.88 -1.96 -0.96]\n",
      " [-2.91 -2.04 -1.    0.  ]]\n",
      "\n",
      "Episode 68's state values\n",
      "[[-4.44 -4.07 -3.67 -2.88]\n",
      " [-4.12 -3.62 -2.8  -1.95]\n",
      " [-3.56 -2.82 -1.92 -0.93]\n",
      " [-2.75 -1.9  -0.96  0.  ]]\n",
      "\n",
      "Episode 69's state values\n",
      "[[-4.55 -4.15 -3.71 -2.99]\n",
      " [-4.19 -3.56 -2.84 -2.02]\n",
      " [-3.63 -2.9  -1.94 -0.97]\n",
      " [-2.94 -1.99 -1.01  0.  ]]\n",
      "\n",
      "Episode 70's state values\n",
      "[[-4.6  -4.26 -3.64 -2.86]\n",
      " [-4.3  -3.66 -2.94 -1.96]\n",
      " [-3.65 -2.9  -2.05 -1.  ]\n",
      " [-2.91 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 71's state values\n",
      "[[-4.6  -4.27 -3.67 -2.91]\n",
      " [-4.27 -3.68 -2.91 -1.99]\n",
      " [-3.68 -2.88 -2.02 -0.98]\n",
      " [-2.88 -1.97 -0.97  0.  ]]\n",
      "\n",
      "Episode 72's state values\n",
      "[[-4.64 -4.26 -3.73 -2.92]\n",
      " [-4.29 -3.73 -2.91 -1.99]\n",
      " [-3.73 -2.93 -1.95 -1.03]\n",
      " [-2.95 -2.03 -1.04  0.  ]]\n",
      "\n",
      "Episode 73's state values\n",
      "[[-4.64 -4.26 -3.69 -2.92]\n",
      " [-4.24 -3.7  -2.92 -1.96]\n",
      " [-3.68 -2.9  -1.96 -0.99]\n",
      " [-2.9  -1.96 -0.96  0.  ]]\n",
      "\n",
      "Episode 74's state values\n",
      "[[-4.63 -4.27 -3.7  -2.93]\n",
      " [-4.27 -3.67 -2.88 -1.97]\n",
      " [-3.67 -2.88 -1.99 -0.98]\n",
      " [-2.9  -1.97 -0.98  0.  ]]\n",
      "\n",
      "Episode 75's state values\n",
      "[[-4.75 -4.35 -3.77 -2.97]\n",
      " [-4.36 -3.76 -2.95 -2.02]\n",
      " [-3.74 -2.94 -1.98 -1.01]\n",
      " [-2.97 -2.   -1.02  0.  ]]\n",
      "\n",
      "Episode 76's state values\n",
      "[[-4.67 -4.3  -3.7  -2.9 ]\n",
      " [-4.3  -3.68 -2.88 -1.95]\n",
      " [-3.67 -2.86 -1.93 -0.96]\n",
      " [-2.94 -1.97 -0.98  0.  ]]\n",
      "\n",
      "Episode 77's state values\n",
      "[[-4.65 -4.28 -3.66 -2.87]\n",
      " [-4.25 -3.65 -2.89 -1.95]\n",
      " [-3.68 -2.85 -1.92 -0.96]\n",
      " [-2.88 -1.98 -1.    0.  ]]\n",
      "\n",
      "Episode 78's state values\n",
      "[[-4.69 -4.32 -3.71 -2.91]\n",
      " [-4.27 -3.7  -2.87 -1.96]\n",
      " [-3.67 -2.89 -1.96 -1.01]\n",
      " [-2.85 -1.92 -0.97  0.  ]]\n",
      "\n",
      "Episode 79's state values\n",
      "[[-4.91 -4.41 -3.88 -3.04]\n",
      " [-4.46 -3.89 -2.93 -2.03]\n",
      " [-3.68 -2.94 -1.97 -1.04]\n",
      " [-3.03 -2.05 -1.01  0.  ]]\n",
      "\n",
      "Episode 80's state values\n",
      "[[-4.94 -4.46 -3.78 -3.02]\n",
      " [-4.44 -3.89 -2.98 -2.02]\n",
      " [-3.94 -3.   -2.05 -0.99]\n",
      " [-3.   -2.06 -1.08  0.  ]]\n",
      "\n",
      "Episode 81's state values\n",
      "[[-4.84 -4.3  -3.61 -2.98]\n",
      " [-4.27 -3.68 -2.8  -2.02]\n",
      " [-3.69 -2.79 -1.88 -0.98]\n",
      " [-2.81 -1.93 -0.91  0.  ]]\n",
      "\n",
      "Episode 82's state values\n",
      "[[-4.75 -4.29 -3.68 -2.86]\n",
      " [-4.27 -3.69 -2.81 -1.88]\n",
      " [-3.64 -2.81 -1.96 -0.94]\n",
      " [-2.88 -1.97 -1.01  0.  ]]\n",
      "\n",
      "Episode 83's state values\n",
      "[[-4.72 -4.38 -3.64 -2.98]\n",
      " [-4.3  -3.61 -2.86 -1.89]\n",
      " [-3.64 -2.79 -2.   -0.92]\n",
      " [-2.9  -1.99 -0.96  0.  ]]\n",
      "\n",
      "Episode 84's state values\n",
      "[[-4.87 -4.42 -3.74 -2.91]\n",
      " [-4.36 -3.73 -2.84 -1.98]\n",
      " [-3.71 -2.87 -1.95 -1.02]\n",
      " [-2.87 -1.91 -0.95  0.  ]]\n",
      "\n",
      "Episode 85's state values\n",
      "[[-4.94 -4.46 -3.78 -2.92]\n",
      " [-4.44 -3.77 -3.   -1.96]\n",
      " [-3.77 -2.93 -1.95 -0.98]\n",
      " [-2.92 -2.02 -1.01  0.  ]]\n",
      "\n",
      "Episode 86's state values\n",
      "[[-4.95 -4.45 -3.78 -2.99]\n",
      " [-4.44 -3.75 -2.94 -1.98]\n",
      " [-3.78 -2.92 -1.94 -0.98]\n",
      " [-2.93 -1.96 -1.01  0.  ]]\n",
      "\n",
      "Episode 87's state values\n",
      "[[-4.93 -4.46 -3.75 -2.96]\n",
      " [-4.42 -3.74 -2.91 -1.96]\n",
      " [-3.76 -2.91 -1.96 -1.  ]\n",
      " [-2.92 -1.97 -1.    0.  ]]\n",
      "\n",
      "Episode 88's state values\n",
      "[[-5.05 -4.56 -3.83 -2.96]\n",
      " [-4.53 -3.82 -2.98 -2.03]\n",
      " [-3.81 -2.97 -1.99 -1.03]\n",
      " [-2.99 -2.   -1.03  0.  ]]\n",
      "\n",
      "Episode 89's state values\n",
      "[[-5.02 -4.52 -3.82 -2.96]\n",
      " [-4.52 -3.81 -2.93 -1.98]\n",
      " [-3.77 -2.91 -1.97 -1.  ]\n",
      " [-2.92 -1.97 -1.    0.  ]]\n",
      "\n",
      "Episode 90's state values\n",
      "[[-5.03 -4.51 -3.81 -2.93]\n",
      " [-4.51 -3.8  -2.93 -1.97]\n",
      " [-3.8  -2.92 -1.96 -0.98]\n",
      " [-2.92 -1.98 -1.01  0.  ]]\n",
      "\n",
      "Episode 91's state values\n",
      "[[-5.04 -4.52 -3.81 -2.94]\n",
      " [-4.51 -3.79 -2.94 -1.97]\n",
      " [-3.8  -2.94 -1.97 -1.  ]\n",
      " [-2.94 -1.97 -0.98  0.  ]]\n",
      "\n",
      "Episode 92's state values\n",
      "[[-5.11 -4.57 -3.85 -2.96]\n",
      " [-4.56 -3.82 -2.96 -1.99]\n",
      " [-3.82 -2.96 -1.99 -1.02]\n",
      " [-2.96 -1.98 -1.01  0.  ]]\n",
      "\n",
      "Episode 93's state values\n",
      "[[-5.12 -4.57 -3.84 -2.95]\n",
      " [-4.58 -3.83 -2.96 -2.  ]\n",
      " [-3.83 -2.95 -1.99 -1.01]\n",
      " [-2.96 -1.99 -1.    0.  ]]\n",
      "\n",
      "Episode 94's state values\n",
      "[[-5.12 -4.56 -3.83 -2.92]\n",
      " [-4.57 -3.82 -2.95 -1.98]\n",
      " [-3.81 -2.93 -1.98 -1.  ]\n",
      " [-2.95 -1.98 -1.    0.  ]]\n",
      "\n",
      "Episode 95's state values\n",
      "[[-5.12 -4.57 -3.84 -2.96]\n",
      " [-4.57 -3.83 -2.94 -1.98]\n",
      " [-3.83 -2.95 -1.98 -0.99]\n",
      " [-2.93 -1.98 -1.    0.  ]]\n",
      "\n",
      "Episode 96's state values\n",
      "[[-5.16 -4.59 -3.86 -2.95]\n",
      " [-4.59 -3.85 -2.95 -1.99]\n",
      " [-3.83 -2.95 -1.99 -1.  ]\n",
      " [-2.96 -1.99 -1.    0.  ]]\n",
      "\n",
      "Episode 97's state values\n",
      "[[-5.17 -4.6  -3.86 -2.94]\n",
      " [-4.6  -3.85 -2.95 -1.98]\n",
      " [-3.84 -2.94 -1.98 -0.99]\n",
      " [-2.96 -1.98 -1.    0.  ]]\n",
      "\n",
      "Episode 98's state values\n",
      "[[-5.19 -4.62 -3.86 -2.97]\n",
      " [-4.61 -3.86 -2.95 -1.99]\n",
      " [-3.85 -2.96 -1.99 -1.  ]\n",
      " [-2.95 -1.98 -1.    0.  ]]\n",
      "\n",
      "Episode 99's state values\n",
      "[[-5.19 -4.62 -3.85 -2.96]\n",
      " [-4.61 -3.85 -2.95 -1.98]\n",
      " [-3.84 -2.96 -1.97 -1.  ]\n",
      " [-2.95 -1.98 -0.99  0.  ]]\n",
      "\n",
      "Episode 100's state values\n",
      "[[-5.22 -4.64 -3.87 -2.96]\n",
      " [-4.63 -3.86 -2.96 -1.99]\n",
      " [-3.86 -2.96 -1.99 -1.  ]\n",
      " [-2.97 -1.99 -1.01  0.  ]]\n",
      "\n",
      "Episode 101's state values\n",
      "[[-5.22 -4.64 -3.86 -2.96]\n",
      " [-4.61 -3.85 -2.95 -1.98]\n",
      " [-3.85 -2.95 -1.97 -0.99]\n",
      " [-2.94 -1.97 -0.99  0.  ]]\n",
      "\n",
      "Episode 102's state values\n",
      "[[-5.25 -4.66 -3.86 -2.97]\n",
      " [-4.63 -3.86 -2.96 -1.99]\n",
      " [-3.85 -2.96 -1.98 -1.  ]\n",
      " [-2.95 -1.98 -1.    0.  ]]\n",
      "\n",
      "Episode 103's state values\n",
      "[[-5.28 -4.66 -3.89 -2.97]\n",
      " [-4.66 -3.88 -2.97 -1.99]\n",
      " [-3.88 -2.98 -1.99 -1.01]\n",
      " [-2.98 -2.   -1.01  0.  ]]\n",
      "\n",
      "Episode 104's state values\n",
      "[[-5.27 -4.65 -3.87 -2.95]\n",
      " [-4.64 -3.86 -2.96 -1.98]\n",
      " [-3.86 -2.96 -1.99 -1.  ]\n",
      " [-2.98 -1.99 -1.    0.  ]]\n",
      "\n",
      "Episode 105's state values\n",
      "[[-5.28 -4.66 -3.86 -2.96]\n",
      " [-4.65 -3.87 -2.95 -1.99]\n",
      " [-3.86 -2.95 -1.98 -1.  ]\n",
      " [-2.94 -1.98 -0.99  0.  ]]\n",
      "\n",
      "Episode 106's state values\n",
      "[[-5.29 -4.67 -3.89 -2.97]\n",
      " [-4.68 -3.88 -2.96 -2.  ]\n",
      " [-3.86 -2.96 -1.99 -1.  ]\n",
      " [-2.95 -1.99 -1.    0.  ]]\n",
      "\n",
      "Episode 107's state values\n",
      "[[-5.34 -4.68 -3.9  -2.96]\n",
      " [-4.7  -3.88 -2.98 -1.99]\n",
      " [-3.9  -2.98 -1.99 -1.  ]\n",
      " [-2.96 -1.99 -1.    0.  ]]\n",
      "\n",
      "Episode 108's state values\n",
      "[[-5.35 -4.72 -3.89 -3.  ]\n",
      " [-4.7  -3.9  -2.96 -2.02]\n",
      " [-3.89 -2.97 -2.   -1.04]\n",
      " [-3.   -2.01 -1.01  0.  ]]\n",
      "\n",
      "Episode 109's state values\n",
      "[[-5.3  -4.69 -3.88 -2.95]\n",
      " [-4.67 -3.86 -2.94 -1.97]\n",
      " [-3.85 -2.95 -1.98 -0.97]\n",
      " [-2.96 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 110's state values\n",
      "[[-5.36 -4.71 -3.89 -2.98]\n",
      " [-4.7  -3.87 -2.97 -1.99]\n",
      " [-3.89 -2.95 -1.98 -1.01]\n",
      " [-2.98 -1.98 -1.02  0.  ]]\n",
      "\n",
      "Episode 111's state values\n",
      "[[-5.39 -4.73 -3.9  -2.96]\n",
      " [-4.72 -3.9  -2.97 -2.  ]\n",
      " [-3.9  -2.98 -1.98 -1.  ]\n",
      " [-3.   -2.   -1.01  0.  ]]\n",
      "\n",
      "Episode 112's state values\n",
      "[[-5.39 -4.71 -3.9  -2.99]\n",
      " [-4.7  -3.89 -2.96 -1.99]\n",
      " [-3.88 -2.97 -2.   -0.99]\n",
      " [-2.95 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 113's state values\n",
      "[[-5.42 -4.72 -3.92 -2.96]\n",
      " [-4.7  -3.9  -2.98 -1.98]\n",
      " [-3.9  -2.96 -2.   -1.01]\n",
      " [-3.   -2.   -1.02  0.  ]]\n",
      "\n",
      "Episode 114's state values\n",
      "[[-5.36 -4.7  -3.88 -2.94]\n",
      " [-4.68 -3.88 -2.97 -1.97]\n",
      " [-3.87 -2.96 -1.98 -0.99]\n",
      " [-2.95 -1.98 -1.01  0.  ]]\n",
      "\n",
      "Episode 115's state values\n",
      "[[-5.39 -4.72 -3.87 -2.94]\n",
      " [-4.72 -3.86 -2.91 -1.98]\n",
      " [-3.87 -2.96 -1.97 -0.98]\n",
      " [-2.93 -1.97 -0.97  0.  ]]\n",
      "\n",
      "Episode 116's state values\n",
      "[[-5.4  -4.74 -3.91 -2.96]\n",
      " [-4.73 -3.91 -2.95 -1.96]\n",
      " [-3.9  -2.96 -1.99 -0.99]\n",
      " [-2.99 -1.99 -1.    0.  ]]\n",
      "\n",
      "Episode 117's state values\n",
      "[[-5.41 -4.77 -3.88 -2.96]\n",
      " [-4.73 -3.9  -2.94 -1.99]\n",
      " [-3.89 -2.94 -1.97 -1.  ]\n",
      " [-2.94 -1.97 -1.    0.  ]]\n",
      "\n",
      "Episode 118's state values\n",
      "[[-5.44 -4.73 -3.9  -2.97]\n",
      " [-4.75 -3.87 -2.97 -1.98]\n",
      " [-3.88 -2.95 -1.98 -0.98]\n",
      " [-3.01 -2.01 -1.02  0.  ]]\n",
      "\n",
      "Episode 119's state values\n",
      "[[-5.43 -4.75 -3.9  -2.95]\n",
      " [-4.74 -3.91 -2.95 -1.99]\n",
      " [-3.88 -3.   -1.96 -1.  ]\n",
      " [-2.97 -1.97 -0.98  0.  ]]\n",
      "\n",
      "Episode 120's state values\n",
      "[[-5.42 -4.75 -3.89 -2.96]\n",
      " [-4.72 -3.88 -2.96 -1.99]\n",
      " [-3.9  -2.93 -1.99 -1.  ]\n",
      " [-2.95 -1.98 -1.02  0.  ]]\n",
      "\n",
      "Episode 121's state values\n",
      "[[-5.5  -4.79 -3.93 -3.01]\n",
      " [-4.79 -3.9  -3.   -2.  ]\n",
      " [-3.91 -2.95 -2.   -1.01]\n",
      " [-3.   -2.01 -1.01  0.  ]]\n",
      "\n",
      "Episode 122's state values\n",
      "[[-5.4  -4.73 -3.91 -2.95]\n",
      " [-4.72 -3.85 -2.93 -1.99]\n",
      " [-3.88 -2.95 -1.95 -0.99]\n",
      " [-2.95 -1.97 -0.98  0.  ]]\n",
      "\n",
      "Episode 123's state values\n",
      "[[-5.42 -4.78 -3.89 -2.99]\n",
      " [-4.72 -3.83 -2.89 -1.98]\n",
      " [-3.84 -2.94 -1.93 -0.98]\n",
      " [-2.91 -1.93 -0.96  0.  ]]\n",
      "\n",
      "Episode 124's state values\n",
      "[[-5.42 -4.78 -3.87 -2.93]\n",
      " [-4.77 -3.86 -2.91 -1.92]\n",
      " [-3.9  -2.97 -1.96 -1.02]\n",
      " [-3.03 -2.04 -1.02  0.  ]]\n",
      "\n",
      "Episode 125's state values\n",
      "[[-5.54 -4.83 -3.86 -2.96]\n",
      " [-4.75 -3.91 -2.96 -1.99]\n",
      " [-3.91 -2.97 -1.98 -1.  ]\n",
      " [-3.01 -2.05 -1.04  0.  ]]\n",
      "\n",
      "Episode 126's state values\n",
      "[[-5.52 -4.77 -3.93 -2.99]\n",
      " [-4.74 -3.91 -2.91 -2.02]\n",
      " [-3.89 -2.93 -1.95 -0.92]\n",
      " [-2.95 -1.96 -0.99  0.  ]]\n",
      "\n",
      "Episode 127's state values\n",
      "[[-5.57 -4.82 -3.97 -3.03]\n",
      " [-4.86 -3.98 -2.99 -2.04]\n",
      " [-3.96 -3.07 -2.09 -1.02]\n",
      " [-3.04 -2.05 -1.04  0.  ]]\n",
      "\n",
      "Episode 128's state values\n",
      "[[-5.51 -4.8  -3.93 -2.94]\n",
      " [-4.75 -3.91 -2.96 -1.98]\n",
      " [-3.92 -2.95 -1.97 -1.  ]\n",
      " [-2.97 -2.   -1.03  0.  ]]\n",
      "\n",
      "Episode 129's state values\n",
      "[[-5.56 -4.82 -3.88 -2.97]\n",
      " [-4.8  -3.91 -3.03 -2.  ]\n",
      " [-3.93 -2.98 -1.93 -1.01]\n",
      " [-3.01 -2.04 -1.04  0.  ]]\n",
      "\n",
      "Episode 130's state values\n",
      "[[-5.55 -4.83 -3.94 -2.98]\n",
      " [-4.8  -3.93 -2.98 -2.  ]\n",
      " [-3.92 -2.97 -2.05 -0.99]\n",
      " [-2.97 -1.99 -1.    0.  ]]\n",
      "\n",
      "Episode 131's state values\n",
      "[[-5.57 -4.81 -3.93 -2.97]\n",
      " [-4.83 -3.94 -3.01 -1.99]\n",
      " [-3.93 -3.   -1.9  -1.03]\n",
      " [-3.01 -2.03 -1.02  0.  ]]\n",
      "\n",
      "Episode 132's state values\n",
      "[[-5.45 -4.72 -3.83 -2.95]\n",
      " [-4.8  -3.86 -2.94 -1.96]\n",
      " [-3.87 -2.91 -1.96 -1.01]\n",
      " [-2.97 -1.98 -0.98  0.  ]]\n",
      "\n",
      "Episode 133's state values\n",
      "[[-5.55 -4.74 -3.92 -2.97]\n",
      " [-4.86 -3.95 -3.01 -2.  ]\n",
      " [-3.95 -2.97 -2.   -1.01]\n",
      " [-2.99 -1.98 -1.    0.  ]]\n",
      "\n",
      "Episode 134's state values\n",
      "[[-5.57 -4.87 -3.94 -3.05]\n",
      " [-4.86 -3.95 -2.96 -2.06]\n",
      " [-3.92 -2.95 -1.99 -1.01]\n",
      " [-2.97 -2.   -1.02  0.  ]]\n",
      "\n",
      "Episode 135's state values\n",
      "[[-5.57 -4.81 -3.93 -3.04]\n",
      " [-4.77 -3.91 -2.95 -2.08]\n",
      " [-3.9  -2.96 -1.96 -1.  ]\n",
      " [-2.94 -1.94 -0.98  0.  ]]\n",
      "\n",
      "Episode 136's state values\n",
      "[[-5.64 -4.84 -3.98 -3.  ]\n",
      " [-4.81 -3.93 -2.98 -1.98]\n",
      " [-3.94 -2.98 -2.01 -1.  ]\n",
      " [-2.96 -1.99 -1.01  0.  ]]\n",
      "\n",
      "Episode 137's state values\n",
      "[[-5.68 -4.89 -3.97 -2.98]\n",
      " [-4.87 -3.99 -3.02 -2.03]\n",
      " [-3.97 -3.01 -2.01 -1.01]\n",
      " [-3.07 -2.03 -1.01  0.  ]]\n",
      "\n",
      "Episode 138's state values\n",
      "[[-5.68 -4.87 -3.96 -3.04]\n",
      " [-4.88 -3.97 -2.98 -2.03]\n",
      " [-3.97 -2.99 -2.01 -1.02]\n",
      " [-3.   -2.01 -1.02  0.  ]]\n",
      "\n",
      "Episode 139's state values\n",
      "[[-5.65 -4.85 -3.91 -3.  ]\n",
      " [-4.84 -3.95 -2.99 -1.98]\n",
      " [-3.94 -2.98 -1.98 -1.01]\n",
      " [-3.   -2.03 -1.01  0.  ]]\n",
      "\n",
      "Episode 140's state values\n",
      "[[-5.59 -4.79 -3.92 -2.97]\n",
      " [-4.84 -3.93 -2.95 -1.97]\n",
      " [-3.94 -2.98 -1.99 -0.98]\n",
      " [-2.97 -1.98 -1.01  0.  ]]\n",
      "\n",
      "Episode 141's state values\n",
      "[[-5.65 -4.84 -3.93 -2.96]\n",
      " [-4.85 -3.93 -2.92 -1.98]\n",
      " [-3.94 -2.96 -1.98 -0.98]\n",
      " [-2.97 -1.98 -1.    0.  ]]\n",
      "\n",
      "Episode 142's state values\n",
      "[[-5.66 -4.87 -3.96 -3.02]\n",
      " [-4.85 -3.95 -2.98 -1.97]\n",
      " [-3.95 -2.98 -1.99 -0.99]\n",
      " [-3.01 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 143's state values\n",
      "[[-5.67 -4.87 -3.94 -2.98]\n",
      " [-4.85 -3.94 -2.99 -1.99]\n",
      " [-3.95 -2.99 -1.99 -1.  ]\n",
      " [-2.98 -2.01 -1.02  0.  ]]\n",
      "\n",
      "Episode 144's state values\n",
      "[[-5.68 -4.86 -3.94 -2.97]\n",
      " [-4.86 -3.95 -2.98 -2.02]\n",
      " [-3.95 -2.99 -2.   -1.01]\n",
      " [-2.99 -2.   -1.01  0.  ]]\n",
      "\n",
      "Episode 145's state values\n",
      "[[-5.7  -4.87 -3.95 -3.  ]\n",
      " [-4.87 -3.96 -2.99 -2.  ]\n",
      " [-3.95 -3.   -2.   -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 146's state values\n",
      "[[-5.69 -4.88 -3.95 -2.99]\n",
      " [-4.87 -3.95 -2.98 -1.99]\n",
      " [-3.96 -2.99 -1.99 -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 147's state values\n",
      "[[-5.67 -4.86 -3.95 -2.98]\n",
      " [-4.87 -3.93 -2.98 -1.99]\n",
      " [-3.95 -2.98 -2.   -1.  ]\n",
      " [-2.98 -2.   -1.01  0.  ]]\n",
      "\n",
      "Episode 148's state values\n",
      "[[-5.69 -4.89 -3.95 -2.98]\n",
      " [-4.88 -3.95 -2.98 -2.  ]\n",
      " [-3.95 -3.   -1.99 -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 149's state values\n",
      "[[-5.71 -4.87 -3.94 -2.98]\n",
      " [-4.86 -3.94 -2.97 -1.99]\n",
      " [-3.93 -2.98 -1.98 -0.99]\n",
      " [-2.96 -1.99 -0.99  0.  ]]\n",
      "\n",
      "Episode 150's state values\n",
      "[[-5.7  -4.86 -3.94 -2.97]\n",
      " [-4.86 -3.94 -2.97 -1.98]\n",
      " [-3.93 -2.97 -1.98 -0.99]\n",
      " [-2.95 -1.98 -0.98  0.  ]]\n",
      "\n",
      "Episode 151's state values\n",
      "[[-5.71 -4.88 -3.95 -2.98]\n",
      " [-4.88 -3.93 -2.96 -1.99]\n",
      " [-3.93 -2.97 -1.98 -0.99]\n",
      " [-2.99 -1.98 -0.99  0.  ]]\n",
      "\n",
      "Episode 152's state values\n",
      "[[-5.71 -4.89 -3.95 -2.99]\n",
      " [-4.9  -3.95 -2.98 -2.  ]\n",
      " [-3.98 -2.99 -1.99 -1.01]\n",
      " [-2.99 -1.99 -1.    0.  ]]\n",
      "\n",
      "Episode 153's state values\n",
      "[[-5.75 -4.89 -3.97 -3.  ]\n",
      " [-4.91 -3.98 -3.   -2.01]\n",
      " [-3.98 -3.   -2.01 -1.01]\n",
      " [-3.   -2.01 -1.01  0.  ]]\n",
      "\n",
      "Episode 154's state values\n",
      "[[-5.72 -4.87 -3.97 -2.97]\n",
      " [-4.89 -3.96 -2.96 -1.99]\n",
      " [-3.95 -2.98 -2.01 -0.99]\n",
      " [-2.99 -1.99 -1.    0.  ]]\n",
      "\n",
      "Episode 155's state values\n",
      "[[-5.7  -4.85 -3.92 -2.97]\n",
      " [-4.85 -3.93 -2.97 -1.99]\n",
      " [-3.91 -2.98 -1.97 -1.  ]\n",
      " [-2.95 -1.97 -0.98  0.  ]]\n",
      "\n",
      "Episode 156's state values\n",
      "[[-5.71 -4.86 -3.94 -2.96]\n",
      " [-4.85 -3.94 -2.99 -1.98]\n",
      " [-3.91 -2.98 -1.97 -0.97]\n",
      " [-2.98 -1.99 -0.99  0.  ]]\n",
      "\n",
      "Episode 157's state values\n",
      "[[-5.79 -4.92 -3.99 -3.01]\n",
      " [-4.94 -4.   -3.01 -2.02]\n",
      " [-3.99 -3.01 -2.   -1.02]\n",
      " [-3.01 -2.02 -1.02  0.  ]]\n",
      "\n",
      "Episode 158's state values\n",
      "[[-5.73 -4.86 -3.89 -2.96]\n",
      " [-4.87 -3.95 -2.97 -1.98]\n",
      " [-3.97 -2.96 -1.98 -1.03]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 159's state values\n",
      "[[-5.73 -4.94 -4.01 -3.  ]\n",
      " [-4.87 -3.96 -2.97 -1.98]\n",
      " [-3.91 -2.99 -1.99 -0.99]\n",
      " [-2.94 -1.97 -0.99  0.  ]]\n",
      "\n",
      "Episode 160's state values\n",
      "[[-5.84 -5.   -4.02 -3.14]\n",
      " [-4.97 -4.04 -2.99 -2.16]\n",
      " [-4.02 -3.05 -2.02 -1.07]\n",
      " [-3.07 -2.04 -1.    0.  ]]\n",
      "\n",
      "Episode 161's state values\n",
      "[[-5.51 -4.66 -3.75 -2.86]\n",
      " [-4.7  -3.78 -2.86 -1.88]\n",
      " [-3.74 -2.83 -1.9  -0.92]\n",
      " [-2.93 -1.97 -0.95  0.  ]]\n",
      "\n",
      "Episode 162's state values\n",
      "[[-5.79 -4.73 -3.98 -2.99]\n",
      " [-4.87 -3.87 -2.99 -1.94]\n",
      " [-4.   -3.01 -2.02 -0.99]\n",
      " [-3.06 -2.   -1.08  0.  ]]\n",
      "\n",
      "Episode 163's state values\n",
      "[[-5.95 -5.1  -4.06 -3.07]\n",
      " [-4.99 -4.04 -3.07 -2.07]\n",
      " [-4.06 -3.05 -2.07 -1.1 ]\n",
      " [-3.03 -2.02 -0.99  0.  ]]\n",
      "\n",
      "Episode 164's state values\n",
      "[[-5.65 -4.92 -3.96 -3.  ]\n",
      " [-4.87 -3.91 -2.9  -1.98]\n",
      " [-3.9  -2.93 -1.96 -0.95]\n",
      " [-2.84 -1.94 -0.98  0.  ]]\n",
      "\n",
      "Episode 165's state values\n",
      "[[-5.69 -4.62 -3.79 -2.9 ]\n",
      " [-4.79 -3.85 -2.89 -1.95]\n",
      " [-3.98 -2.92 -1.88 -0.98]\n",
      " [-3.1  -2.05 -1.05  0.  ]]\n",
      "\n",
      "Episode 166's state values\n",
      "[[-5.93 -4.84 -3.95 -2.87]\n",
      " [-4.99 -4.03 -3.14 -2.11]\n",
      " [-4.19 -3.05 -2.   -1.08]\n",
      " [-3.24 -2.16 -1.05  0.  ]]\n",
      "\n",
      "Episode 167's state values\n",
      "[[-5.75 -4.84 -3.93 -3.01]\n",
      " [-4.82 -3.85 -2.93 -2.01]\n",
      " [-3.88 -2.93 -2.01 -0.98]\n",
      " [-2.99 -2.   -1.06  0.  ]]\n",
      "\n",
      "Episode 168's state values\n",
      "[[-5.75 -4.98 -4.   -3.04]\n",
      " [-4.79 -3.91 -2.85 -1.93]\n",
      " [-3.77 -2.99 -1.98 -0.93]\n",
      " [-2.93 -1.97 -1.    0.  ]]\n",
      "\n",
      "Episode 169's state values\n",
      "[[-5.83 -4.92 -3.98 -2.94]\n",
      " [-4.87 -3.9  -2.96 -1.96]\n",
      " [-4.07 -2.91 -1.87 -0.98]\n",
      " [-2.92 -1.94 -0.92  0.  ]]\n",
      "\n",
      "Episode 170's state values\n",
      "[[-5.88 -4.86 -3.87 -2.98]\n",
      " [-5.02 -4.02 -2.99 -2.08]\n",
      " [-3.93 -3.01 -2.   -1.04]\n",
      " [-3.05 -2.01 -1.06  0.  ]]\n",
      "\n",
      "Episode 171's state values\n",
      "[[-5.71 -4.77 -3.8  -2.92]\n",
      " [-4.79 -3.94 -2.9  -1.97]\n",
      " [-3.97 -2.92 -1.95 -0.9 ]\n",
      " [-2.92 -1.95 -0.96  0.  ]]\n",
      "\n",
      "Episode 172's state values\n",
      "[[-5.69 -4.95 -3.8  -3.  ]\n",
      " [-4.85 -3.89 -2.95 -1.89]\n",
      " [-3.86 -2.93 -1.95 -1.  ]\n",
      " [-2.94 -1.99 -0.99  0.  ]]\n",
      "\n",
      "Episode 173's state values\n",
      "[[-5.7  -4.79 -3.86 -2.99]\n",
      " [-4.92 -3.87 -2.89 -1.96]\n",
      " [-3.96 -2.96 -1.9  -0.99]\n",
      " [-2.97 -1.96 -0.98  0.  ]]\n",
      "\n",
      "Episode 174's state values\n",
      "[[-5.74 -4.89 -4.   -2.95]\n",
      " [-4.97 -3.91 -2.95 -2.  ]\n",
      " [-3.96 -3.02 -1.98 -1.01]\n",
      " [-3.   -1.99 -1.01  0.  ]]\n",
      "\n",
      "Episode 175's state values\n",
      "[[-5.79 -4.88 -3.92 -2.98]\n",
      " [-4.86 -3.98 -3.   -2.01]\n",
      " [-3.95 -2.98 -1.97 -1.01]\n",
      " [-2.95 -1.98 -1.    0.  ]]\n",
      "\n",
      "Episode 176's state values\n",
      "[[-5.77 -4.9  -3.97 -2.95]\n",
      " [-4.88 -3.93 -2.89 -1.96]\n",
      " [-3.96 -2.95 -1.95 -0.99]\n",
      " [-2.98 -2.01 -0.99  0.  ]]\n",
      "\n",
      "Episode 177's state values\n",
      "[[-5.85 -4.92 -3.93 -3.04]\n",
      " [-4.97 -3.95 -3.   -2.01]\n",
      " [-3.97 -3.02 -2.   -1.01]\n",
      " [-3.   -1.94 -0.99  0.  ]]\n",
      "\n",
      "Episode 178's state values\n",
      "[[-5.8  -4.92 -3.99 -3.01]\n",
      " [-4.86 -3.93 -2.99 -1.99]\n",
      " [-3.95 -2.98 -1.97 -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 179's state values\n",
      "[[-5.81 -4.91 -3.96 -3.  ]\n",
      " [-4.91 -3.96 -2.99 -2.01]\n",
      " [-3.98 -2.99 -1.99 -1.02]\n",
      " [-3.02 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 180's state values\n",
      "[[-5.84 -4.93 -3.96 -3.  ]\n",
      " [-4.94 -3.97 -2.98 -2.03]\n",
      " [-3.97 -3.01 -1.99 -0.99]\n",
      " [-2.97 -2.01 -1.02  0.  ]]\n",
      "\n",
      "Episode 181's state values\n",
      "[[-5.85 -4.96 -3.96 -3.  ]\n",
      " [-4.93 -3.95 -3.   -1.98]\n",
      " [-3.96 -2.97 -1.99 -1.02]\n",
      " [-3.   -1.98 -0.98  0.  ]]\n",
      "\n",
      "Episode 182's state values\n",
      "[[-5.83 -4.91 -3.98 -3.02]\n",
      " [-4.92 -3.97 -2.96 -2.02]\n",
      " [-3.95 -2.99 -1.97 -0.99]\n",
      " [-2.96 -1.99 -1.    0.  ]]\n",
      "\n",
      "Episode 183's state values\n",
      "[[-5.86 -4.94 -3.97 -2.97]\n",
      " [-4.92 -3.96 -3.02 -1.97]\n",
      " [-4.   -3.   -2.01 -1.02]\n",
      " [-3.03 -2.01 -1.01  0.  ]]\n",
      "\n",
      "Episode 184's state values\n",
      "[[-5.84 -4.94 -3.97 -2.99]\n",
      " [-4.95 -3.96 -2.97 -2.03]\n",
      " [-3.97 -2.99 -1.98 -0.99]\n",
      " [-2.98 -2.01 -1.    0.  ]]\n",
      "\n",
      "Episode 185's state values\n",
      "[[-5.81 -4.89 -3.94 -3.  ]\n",
      " [-4.88 -3.92 -2.97 -1.98]\n",
      " [-3.94 -2.96 -1.98 -1.  ]\n",
      " [-2.96 -1.98 -0.99  0.  ]]\n",
      "\n",
      "Episode 186's state values\n",
      "[[-5.83 -4.9  -3.94 -2.96]\n",
      " [-4.91 -3.95 -2.96 -1.96]\n",
      " [-3.95 -2.97 -1.98 -0.99]\n",
      " [-2.97 -1.97 -1.    0.  ]]\n",
      "\n",
      "Episode 187's state values\n",
      "[[-5.83 -4.92 -3.95 -3.01]\n",
      " [-4.92 -3.94 -2.98 -2.  ]\n",
      " [-3.97 -2.98 -1.97 -1.  ]\n",
      " [-2.99 -1.98 -0.99  0.  ]]\n",
      "\n",
      "Episode 188's state values\n",
      "[[-5.85 -4.92 -3.97 -2.96]\n",
      " [-4.93 -3.96 -2.99 -2.  ]\n",
      " [-3.96 -2.99 -2.   -1.01]\n",
      " [-2.98 -2.01 -1.01  0.  ]]\n",
      "\n",
      "Episode 189's state values\n",
      "[[-5.86 -4.92 -3.95 -3.02]\n",
      " [-4.94 -3.97 -2.98 -1.98]\n",
      " [-3.98 -3.   -2.   -1.  ]\n",
      " [-3.01 -2.01 -1.01  0.  ]]\n",
      "\n",
      "Episode 190's state values\n",
      "[[-5.86 -4.93 -3.94 -2.96]\n",
      " [-4.94 -3.96 -2.99 -1.99]\n",
      " [-3.97 -2.98 -1.98 -1.  ]\n",
      " [-2.99 -1.99 -1.01  0.  ]]\n",
      "\n",
      "Episode 191's state values\n",
      "[[-5.85 -4.91 -3.95 -2.98]\n",
      " [-4.92 -3.95 -2.97 -1.98]\n",
      " [-3.96 -2.98 -1.99 -0.99]\n",
      " [-2.98 -1.98 -0.99  0.  ]]\n",
      "\n",
      "Episode 192's state values\n",
      "[[-5.84 -4.92 -3.96 -2.99]\n",
      " [-4.92 -3.95 -2.98 -1.98]\n",
      " [-3.97 -2.98 -1.98 -1.  ]\n",
      " [-2.98 -1.99 -1.    0.  ]]\n",
      "\n",
      "Episode 193's state values\n",
      "[[-5.86 -4.92 -3.95 -2.98]\n",
      " [-4.93 -3.96 -2.98 -1.98]\n",
      " [-3.97 -2.99 -1.99 -1.  ]\n",
      " [-2.99 -1.99 -1.    0.  ]]\n",
      "\n",
      "Episode 194's state values\n",
      "[[-5.85 -4.93 -3.96 -2.98]\n",
      " [-4.93 -3.96 -2.98 -1.99]\n",
      " [-3.96 -2.98 -1.99 -1.  ]\n",
      " [-2.99 -1.99 -1.    0.  ]]\n",
      "\n",
      "Episode 195's state values\n",
      "[[-5.86 -4.92 -3.96 -2.98]\n",
      " [-4.93 -3.95 -2.98 -1.98]\n",
      " [-3.97 -2.98 -1.98 -1.  ]\n",
      " [-2.99 -1.99 -1.    0.  ]]\n",
      "\n",
      "Episode 196's state values\n",
      "[[-5.86 -4.93 -3.96 -2.98]\n",
      " [-4.93 -3.96 -2.98 -1.99]\n",
      " [-3.97 -2.98 -1.99 -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 197's state values\n",
      "[[-5.86 -4.93 -3.96 -2.98]\n",
      " [-4.93 -3.96 -2.98 -1.99]\n",
      " [-3.97 -2.98 -1.99 -1.  ]\n",
      " [-2.99 -1.99 -1.    0.  ]]\n",
      "\n",
      "Episode 198's state values\n",
      "[[-5.86 -4.93 -3.96 -2.98]\n",
      " [-4.93 -3.96 -2.98 -1.99]\n",
      " [-3.97 -2.98 -1.99 -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 199's state values\n",
      "[[-5.87 -4.93 -3.96 -2.98]\n",
      " [-4.93 -3.96 -2.98 -1.99]\n",
      " [-3.97 -2.98 -1.99 -1.  ]\n",
      " [-2.99 -1.99 -1.    0.  ]]\n",
      "\n",
      "Episode 200's state values\n",
      "[[-5.87 -4.93 -3.96 -2.98]\n",
      " [-4.93 -3.96 -2.98 -1.99]\n",
      " [-3.97 -2.98 -1.99 -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 201's state values\n",
      "[[-5.87 -4.93 -3.96 -2.98]\n",
      " [-4.93 -3.96 -2.98 -1.99]\n",
      " [-3.97 -2.98 -1.99 -1.  ]\n",
      " [-2.99 -1.99 -1.    0.  ]]\n",
      "\n",
      "Episode 202's state values\n",
      "[[-5.87 -4.93 -3.96 -2.98]\n",
      " [-4.93 -3.96 -2.98 -1.99]\n",
      " [-3.97 -2.98 -1.99 -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 203's state values\n",
      "[[-5.88 -4.94 -3.96 -2.98]\n",
      " [-4.93 -3.96 -2.98 -1.99]\n",
      " [-3.97 -2.98 -1.99 -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 204's state values\n",
      "[[-5.87 -4.93 -3.96 -2.98]\n",
      " [-4.93 -3.96 -2.98 -1.99]\n",
      " [-3.97 -2.98 -1.99 -1.  ]\n",
      " [-2.99 -1.99 -1.    0.  ]]\n",
      "\n",
      "Episode 205's state values\n",
      "[[-5.88 -4.93 -3.96 -2.98]\n",
      " [-4.94 -3.96 -2.98 -1.99]\n",
      " [-3.97 -2.98 -1.99 -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 206's state values\n",
      "[[-5.88 -4.93 -3.96 -2.98]\n",
      " [-4.94 -3.96 -2.98 -1.99]\n",
      " [-3.97 -2.98 -1.99 -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 207's state values\n",
      "[[-5.88 -4.94 -3.97 -2.98]\n",
      " [-4.94 -3.96 -2.98 -1.99]\n",
      " [-3.97 -2.98 -1.99 -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 208's state values\n",
      "[[-5.88 -4.94 -3.97 -2.99]\n",
      " [-4.94 -3.97 -2.98 -1.99]\n",
      " [-3.97 -2.99 -1.99 -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 209's state values\n",
      "[[-5.88 -4.94 -3.96 -2.98]\n",
      " [-4.94 -3.96 -2.98 -1.99]\n",
      " [-3.97 -2.98 -1.99 -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 210's state values\n",
      "[[-5.88 -4.94 -3.97 -2.98]\n",
      " [-4.94 -3.97 -2.98 -1.99]\n",
      " [-3.97 -2.98 -1.99 -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 211's state values\n",
      "[[-5.89 -4.94 -3.97 -2.98]\n",
      " [-4.94 -3.97 -2.98 -1.99]\n",
      " [-3.97 -2.98 -1.99 -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 212's state values\n",
      "[[-5.89 -4.94 -3.97 -2.98]\n",
      " [-4.94 -3.97 -2.98 -1.99]\n",
      " [-3.98 -2.99 -1.99 -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 213's state values\n",
      "[[-5.89 -4.94 -3.97 -2.98]\n",
      " [-4.94 -3.97 -2.98 -1.99]\n",
      " [-3.98 -2.99 -1.99 -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 214's state values\n",
      "[[-5.9  -4.94 -3.97 -2.98]\n",
      " [-4.94 -3.97 -2.98 -1.99]\n",
      " [-3.98 -2.98 -1.99 -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 215's state values\n",
      "[[-5.9  -4.95 -3.97 -2.98]\n",
      " [-4.95 -3.97 -2.98 -1.99]\n",
      " [-3.98 -2.99 -1.99 -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 216's state values\n",
      "[[-5.9  -4.94 -3.97 -2.98]\n",
      " [-4.95 -3.97 -2.98 -1.99]\n",
      " [-3.98 -2.98 -1.99 -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 217's state values\n",
      "[[-5.9  -4.95 -3.97 -2.99]\n",
      " [-4.95 -3.97 -2.99 -1.99]\n",
      " [-3.98 -2.99 -1.99 -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 218's state values\n",
      "[[-5.9  -4.95 -3.97 -2.99]\n",
      " [-4.95 -3.97 -2.99 -1.99]\n",
      " [-3.98 -2.99 -1.99 -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 219's state values\n",
      "[[-5.9  -4.95 -3.97 -2.99]\n",
      " [-4.95 -3.97 -2.99 -2.  ]\n",
      " [-3.98 -2.99 -1.99 -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 220's state values\n",
      "[[-5.9  -4.95 -3.97 -2.99]\n",
      " [-4.95 -3.97 -2.99 -1.99]\n",
      " [-3.98 -2.99 -2.   -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 221's state values\n",
      "[[-5.91 -4.95 -3.97 -2.99]\n",
      " [-4.95 -3.97 -2.99 -1.99]\n",
      " [-3.98 -2.99 -2.   -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 222's state values\n",
      "[[-5.91 -4.95 -3.97 -2.99]\n",
      " [-4.95 -3.97 -2.99 -1.99]\n",
      " [-3.98 -2.99 -1.99 -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 223's state values\n",
      "[[-5.91 -4.95 -3.97 -2.99]\n",
      " [-4.95 -3.97 -2.99 -2.  ]\n",
      " [-3.98 -2.99 -1.99 -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 224's state values\n",
      "[[-5.91 -4.95 -3.97 -2.99]\n",
      " [-4.95 -3.97 -2.99 -1.99]\n",
      " [-3.98 -2.99 -2.   -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 225's state values\n",
      "[[-5.91 -4.95 -3.97 -2.99]\n",
      " [-4.95 -3.97 -2.99 -2.  ]\n",
      " [-3.98 -2.99 -2.   -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 226's state values\n",
      "[[-5.91 -4.95 -3.97 -2.99]\n",
      " [-4.95 -3.97 -2.99 -1.99]\n",
      " [-3.98 -2.99 -1.99 -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 227's state values\n",
      "[[-5.91 -4.95 -3.98 -2.99]\n",
      " [-4.95 -3.97 -2.99 -2.  ]\n",
      " [-3.98 -2.99 -2.   -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 228's state values\n",
      "[[-5.91 -4.95 -3.97 -2.99]\n",
      " [-4.95 -3.97 -2.99 -2.  ]\n",
      " [-3.98 -2.99 -1.99 -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 229's state values\n",
      "[[-5.91 -4.95 -3.98 -2.99]\n",
      " [-4.95 -3.97 -2.99 -2.  ]\n",
      " [-3.98 -2.99 -2.   -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 230's state values\n",
      "[[-5.92 -4.96 -3.97 -2.99]\n",
      " [-4.95 -3.98 -2.99 -2.  ]\n",
      " [-3.98 -2.99 -2.   -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 231's state values\n",
      "[[-5.92 -4.96 -3.98 -2.99]\n",
      " [-4.96 -3.98 -2.99 -2.  ]\n",
      " [-3.98 -2.99 -2.   -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 232's state values\n",
      "[[-5.92 -4.96 -3.98 -2.99]\n",
      " [-4.96 -3.98 -2.99 -2.  ]\n",
      " [-3.98 -2.99 -2.   -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 233's state values\n",
      "[[-5.92 -4.96 -3.98 -2.99]\n",
      " [-4.96 -3.98 -2.99 -2.  ]\n",
      " [-3.98 -2.99 -2.   -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 234's state values\n",
      "[[-5.92 -4.96 -3.98 -2.99]\n",
      " [-4.96 -3.98 -2.99 -2.  ]\n",
      " [-3.98 -2.99 -2.   -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 235's state values\n",
      "[[-5.92 -4.96 -3.98 -2.99]\n",
      " [-4.96 -3.98 -2.99 -2.  ]\n",
      " [-3.98 -2.99 -2.   -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 236's state values\n",
      "[[-5.92 -4.96 -3.98 -2.99]\n",
      " [-4.96 -3.98 -2.99 -2.  ]\n",
      " [-3.98 -2.99 -2.   -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 237's state values\n",
      "[[-5.92 -4.96 -3.98 -2.99]\n",
      " [-4.96 -3.98 -2.99 -2.  ]\n",
      " [-3.98 -2.99 -2.   -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 238's state values\n",
      "[[-5.93 -4.96 -3.98 -2.99]\n",
      " [-4.96 -3.98 -2.99 -2.  ]\n",
      " [-3.98 -2.99 -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 239's state values\n",
      "[[-5.93 -4.96 -3.98 -2.99]\n",
      " [-4.96 -3.98 -2.99 -2.  ]\n",
      " [-3.98 -2.99 -2.   -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 240's state values\n",
      "[[-5.93 -4.96 -3.98 -2.99]\n",
      " [-4.96 -3.98 -2.99 -2.  ]\n",
      " [-3.98 -2.99 -2.   -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 241's state values\n",
      "[[-5.93 -4.96 -3.98 -2.99]\n",
      " [-4.96 -3.98 -2.99 -2.  ]\n",
      " [-3.98 -2.99 -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 242's state values\n",
      "[[-5.93 -4.96 -3.98 -2.99]\n",
      " [-4.96 -3.98 -2.99 -2.  ]\n",
      " [-3.98 -2.99 -2.   -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 243's state values\n",
      "[[-5.93 -4.96 -3.98 -2.99]\n",
      " [-4.96 -3.98 -2.99 -2.  ]\n",
      " [-3.98 -2.99 -2.   -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 244's state values\n",
      "[[-5.93 -4.96 -3.98 -2.99]\n",
      " [-4.96 -3.98 -2.99 -2.  ]\n",
      " [-3.98 -2.99 -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 245's state values\n",
      "[[-5.93 -4.96 -3.98 -2.99]\n",
      " [-4.96 -3.98 -2.99 -2.  ]\n",
      " [-3.98 -2.99 -2.   -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 246's state values\n",
      "[[-5.93 -4.97 -3.98 -2.99]\n",
      " [-4.96 -3.98 -2.99 -2.  ]\n",
      " [-3.98 -2.99 -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 247's state values\n",
      "[[-5.93 -4.97 -3.98 -2.99]\n",
      " [-4.97 -3.98 -2.99 -2.  ]\n",
      " [-3.98 -2.99 -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 248's state values\n",
      "[[-5.93 -4.97 -3.98 -2.99]\n",
      " [-4.97 -3.98 -2.99 -2.  ]\n",
      " [-3.99 -2.99 -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 249's state values\n",
      "[[-5.94 -4.97 -3.98 -2.99]\n",
      " [-4.97 -3.98 -2.99 -2.  ]\n",
      " [-3.99 -3.   -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 250's state values\n",
      "[[-5.94 -4.97 -3.98 -2.99]\n",
      " [-4.97 -3.98 -2.99 -2.  ]\n",
      " [-3.99 -2.99 -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 251's state values\n",
      "[[-5.94 -4.97 -3.98 -2.99]\n",
      " [-4.97 -3.98 -2.99 -2.  ]\n",
      " [-3.99 -2.99 -2.   -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 252's state values\n",
      "[[-5.94 -4.97 -3.98 -2.99]\n",
      " [-4.97 -3.98 -2.99 -2.  ]\n",
      " [-3.98 -2.99 -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 253's state values\n",
      "[[-5.94 -4.97 -3.98 -2.99]\n",
      " [-4.97 -3.99 -2.99 -2.  ]\n",
      " [-3.99 -2.99 -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 254's state values\n",
      "[[-5.94 -4.97 -3.98 -2.99]\n",
      " [-4.97 -3.98 -2.99 -2.  ]\n",
      " [-3.99 -2.99 -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 255's state values\n",
      "[[-5.94 -4.97 -3.98 -2.99]\n",
      " [-4.97 -3.98 -2.99 -2.  ]\n",
      " [-3.99 -2.99 -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 256's state values\n",
      "[[-5.94 -4.97 -3.98 -2.99]\n",
      " [-4.97 -3.98 -2.99 -2.  ]\n",
      " [-3.99 -2.99 -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 257's state values\n",
      "[[-5.95 -4.97 -3.99 -2.99]\n",
      " [-4.97 -3.99 -2.99 -2.  ]\n",
      " [-3.99 -2.99 -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 258's state values\n",
      "[[-5.95 -4.97 -3.99 -3.  ]\n",
      " [-4.97 -3.99 -2.99 -2.  ]\n",
      " [-3.99 -2.99 -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 259's state values\n",
      "[[-5.94 -4.97 -3.99 -2.99]\n",
      " [-4.97 -3.99 -2.99 -2.  ]\n",
      " [-3.99 -2.99 -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 260's state values\n",
      "[[-5.94 -4.97 -3.99 -3.  ]\n",
      " [-4.97 -3.99 -2.99 -2.  ]\n",
      " [-3.99 -2.99 -2.   -0.99]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 261's state values\n",
      "[[-5.94 -4.97 -3.98 -2.99]\n",
      " [-4.97 -3.98 -2.99 -2.  ]\n",
      " [-3.98 -2.99 -1.99 -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 262's state values\n",
      "[[-5.95 -4.97 -3.99 -2.99]\n",
      " [-4.97 -3.98 -2.99 -2.  ]\n",
      " [-3.99 -2.99 -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 263's state values\n",
      "[[-5.95 -4.97 -3.98 -3.  ]\n",
      " [-4.97 -3.99 -2.99 -2.  ]\n",
      " [-3.99 -3.   -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 264's state values\n",
      "[[-5.95 -4.97 -3.99 -3.  ]\n",
      " [-4.97 -3.99 -3.   -2.  ]\n",
      " [-3.99 -3.   -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 265's state values\n",
      "[[-5.95 -4.97 -3.99 -3.  ]\n",
      " [-4.98 -3.99 -2.99 -2.  ]\n",
      " [-4.   -3.   -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 266's state values\n",
      "[[-5.95 -4.98 -3.99 -2.99]\n",
      " [-4.98 -3.99 -3.   -2.  ]\n",
      " [-3.99 -3.   -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 267's state values\n",
      "[[-5.95 -4.97 -3.99 -3.  ]\n",
      " [-4.97 -3.99 -3.   -2.  ]\n",
      " [-3.99 -2.99 -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 268's state values\n",
      "[[-5.95 -4.97 -3.98 -2.99]\n",
      " [-4.97 -3.99 -2.99 -2.  ]\n",
      " [-3.99 -3.   -2.   -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 269's state values\n",
      "[[-5.94 -4.97 -3.98 -3.  ]\n",
      " [-4.97 -3.98 -2.99 -1.99]\n",
      " [-3.99 -2.99 -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 270's state values\n",
      "[[-5.95 -4.97 -3.99 -2.99]\n",
      " [-4.98 -3.99 -2.99 -2.  ]\n",
      " [-3.99 -2.99 -2.   -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 271's state values\n",
      "[[-5.96 -4.98 -3.99 -3.  ]\n",
      " [-4.98 -3.99 -3.   -2.  ]\n",
      " [-3.99 -3.   -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 272's state values\n",
      "[[-5.95 -4.97 -3.99 -2.99]\n",
      " [-4.97 -3.99 -2.99 -2.  ]\n",
      " [-3.99 -2.99 -2.   -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 273's state values\n",
      "[[-5.96 -4.97 -3.99 -3.  ]\n",
      " [-4.97 -3.98 -2.99 -1.99]\n",
      " [-3.99 -2.99 -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 274's state values\n",
      "[[-5.95 -4.98 -3.99 -2.99]\n",
      " [-4.98 -3.99 -3.   -2.  ]\n",
      " [-3.99 -3.   -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 275's state values\n",
      "[[-5.96 -4.98 -3.99 -3.  ]\n",
      " [-4.98 -3.99 -3.   -2.  ]\n",
      " [-3.99 -3.   -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 276's state values\n",
      "[[-5.96 -4.98 -3.99 -2.99]\n",
      " [-4.98 -3.99 -3.   -2.  ]\n",
      " [-3.99 -3.   -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 277's state values\n",
      "[[-5.96 -4.98 -3.99 -3.  ]\n",
      " [-4.98 -3.99 -2.99 -2.  ]\n",
      " [-3.99 -3.   -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 278's state values\n",
      "[[-5.96 -4.98 -3.99 -3.  ]\n",
      " [-4.98 -3.99 -3.   -2.  ]\n",
      " [-3.99 -3.   -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 279's state values\n",
      "[[-5.96 -4.98 -3.99 -3.  ]\n",
      " [-4.98 -3.99 -3.   -2.  ]\n",
      " [-3.99 -3.   -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 280's state values\n",
      "[[-5.96 -4.98 -3.99 -2.99]\n",
      " [-4.98 -3.99 -3.   -2.  ]\n",
      " [-3.99 -3.   -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 281's state values\n",
      "[[-5.96 -4.98 -3.99 -3.01]\n",
      " [-4.98 -3.99 -2.99 -2.  ]\n",
      " [-3.99 -2.99 -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 282's state values\n",
      "[[-5.96 -4.98 -3.99 -3.  ]\n",
      " [-4.98 -3.99 -3.   -2.  ]\n",
      " [-3.99 -3.   -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 283's state values\n",
      "[[-5.96 -4.98 -3.99 -3.  ]\n",
      " [-4.98 -3.99 -3.   -2.  ]\n",
      " [-3.99 -3.   -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 284's state values\n",
      "[[-5.96 -4.98 -3.99 -3.  ]\n",
      " [-4.98 -3.99 -3.   -2.  ]\n",
      " [-3.99 -3.   -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 285's state values\n",
      "[[-5.96 -4.98 -3.99 -3.  ]\n",
      " [-4.98 -3.99 -3.   -2.  ]\n",
      " [-3.99 -3.   -2.   -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 286's state values\n",
      "[[-5.96 -4.98 -3.99 -3.  ]\n",
      " [-4.98 -3.99 -3.   -2.  ]\n",
      " [-3.99 -3.   -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 287's state values\n",
      "[[-5.96 -4.98 -3.99 -2.99]\n",
      " [-4.98 -3.99 -3.   -2.  ]\n",
      " [-3.99 -2.99 -2.   -1.  ]\n",
      " [-2.99 -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 288's state values\n",
      "[[-5.96 -4.98 -3.99 -2.99]\n",
      " [-4.98 -3.99 -2.99 -2.  ]\n",
      " [-3.99 -2.99 -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 289's state values\n",
      "[[-5.96 -4.98 -3.99 -3.  ]\n",
      " [-4.98 -3.99 -3.   -2.  ]\n",
      " [-3.99 -3.   -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 290's state values\n",
      "[[-5.96 -4.98 -3.99 -3.  ]\n",
      " [-4.98 -3.99 -3.   -2.  ]\n",
      " [-3.99 -3.   -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 291's state values\n",
      "[[-5.97 -4.98 -3.99 -3.  ]\n",
      " [-4.99 -3.99 -3.   -2.  ]\n",
      " [-3.99 -3.   -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 292's state values\n",
      "[[-5.97 -4.98 -3.99 -3.  ]\n",
      " [-4.98 -3.99 -3.   -2.  ]\n",
      " [-3.99 -3.   -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 293's state values\n",
      "[[-5.97 -4.98 -3.99 -3.  ]\n",
      " [-4.98 -3.99 -3.   -2.  ]\n",
      " [-3.99 -3.   -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 294's state values\n",
      "[[-5.97 -4.98 -3.99 -3.  ]\n",
      " [-4.98 -3.99 -3.   -2.  ]\n",
      " [-3.99 -3.   -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 295's state values\n",
      "[[-5.97 -4.98 -3.99 -3.  ]\n",
      " [-4.98 -3.99 -3.   -2.  ]\n",
      " [-3.99 -3.   -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 296's state values\n",
      "[[-5.97 -4.98 -3.99 -3.  ]\n",
      " [-4.98 -3.99 -3.   -2.  ]\n",
      " [-3.99 -3.   -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 297's state values\n",
      "[[-5.97 -4.98 -3.99 -3.  ]\n",
      " [-4.98 -3.99 -3.   -2.  ]\n",
      " [-3.99 -3.   -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 298's state values\n",
      "[[-5.97 -4.99 -3.99 -3.  ]\n",
      " [-4.99 -3.99 -3.   -2.  ]\n",
      " [-4.   -3.   -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "\n",
      "Episode 299's state values\n",
      "[[-5.97 -4.99 -3.99 -3.  ]\n",
      " [-4.98 -3.99 -3.   -2.  ]\n",
      " [-4.   -3.   -2.   -1.  ]\n",
      " [-3.   -2.   -1.    0.  ]]\n",
      "Episode 299's policy\n",
      "[[ 3.  1.  1.  1.]\n",
      " [ 3.  3.  3.  1.]\n",
      " [ 3.  3.  3.  1.]\n",
      " [ 3.  3.  3. 10.]]\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 300\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = state_encoder(state)\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        observation = state_encoder(observation)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print_state_values(num_states, i_episode)\n",
    "\n",
    "# Extract policy\n",
    "extract_policy(num_states, i_episode)\n",
    "\n",
    "print('Complete')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kalman_bar_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
