{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-learning using TAGIV on small grdiworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "from src.gym_kalman.env_Gridworld import GridworldEnv\n",
    "from pytagi.nn import Linear, OutputUpdater, ReLU, Sequential, EvenExp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize value function\n",
    "import numpy as np\n",
    "\n",
    "# initialize the environment\n",
    "grid_size = 4\n",
    "env = GridworldEnv(grid_size=grid_size, reward_std=0.2)\n",
    "num_states = env.observation_space.n\n",
    "actions = np.arange(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    # \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TAGI_Net():\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(TAGI_Net, self).__init__()\n",
    "        self.net = Sequential(\n",
    "                    Linear(n_observations, 128),\n",
    "                    ReLU(),\n",
    "                    Linear(128, 128),\n",
    "                    ReLU(),\n",
    "                    Linear(128, n_actions * 2),\n",
    "                    EvenExp()\n",
    "                    )\n",
    "        self.n_actions = n_actions\n",
    "        self.n_observations = n_observations\n",
    "    def forward(self, mu_x, var_x):\n",
    "        return self.net.forward(mu_x, var_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 10\n",
    "GAMMA = 1\n",
    "\n",
    "TAU = 0.005\n",
    "LR = 1e-2\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = 1\n",
    "\n",
    "policy_net = TAGI_Net(n_observations, n_actions)\n",
    "target_net = TAGI_Net(n_observations, n_actions)\n",
    "target_net.net.load_state_dict(policy_net.net.get_state_dict())\n",
    "\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "# # Thompson Sampling\n",
    "# def select_action(state):\n",
    "#     global steps_done\n",
    "#     steps_done += 1\n",
    "#     policy_net.net.eval()\n",
    "#     state_np = state.numpy()\n",
    "\n",
    "#     state_temp = np.array(state_np)\n",
    "#     state_np = np.repeat(state_temp, BATCH_SIZE, axis=0)\n",
    "\n",
    "#     ma, Sa = policy_net.net(state_np)\n",
    "#     ma = ma.reshape(BATCH_SIZE, policy_net.n_actions*2)\n",
    "#     ma = ma[0]\n",
    "#     action_mean = ma[::2]\n",
    "#     Sa = Sa.reshape(BATCH_SIZE, policy_net.n_actions*2)[0]\n",
    "#     action_var = Sa[::2] + ma[1::2]\n",
    "\n",
    "#     a_sample = np.zeros_like(action_mean)\n",
    "#     # print(state)\n",
    "#     for i in range(len(action_mean)):\n",
    "#         a_sample[i] = np.random.normal(action_mean[i], np.sqrt(action_var[i]))\n",
    "\n",
    "#     action = np.argmax(a_sample, axis=0)\n",
    "\n",
    "#     steps_done += 1\n",
    "#     return torch.tensor([[action]],device=device)\n",
    "\n",
    "# Greedy\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.0001\n",
    "EPS_DECAY = 1000\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "\n",
    "    policy_net.net.eval()\n",
    "    state_np = state.numpy()\n",
    "\n",
    "    state_temp = np.array(state_np)\n",
    "    state_np = np.repeat(state_temp, BATCH_SIZE, axis=0)\n",
    "\n",
    "    ma, Sa = policy_net.net(state_np)\n",
    "    ma = ma.reshape(BATCH_SIZE, policy_net.n_actions*2)\n",
    "    ma = ma[0]\n",
    "    action_mean = ma[::2]\n",
    "    Sa = Sa.reshape(BATCH_SIZE, policy_net.n_actions*2)[0]\n",
    "    action_var = Sa[::2] + ma[1::2]\n",
    "\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        action = np.argmax(action_mean, axis=0)\n",
    "    else:\n",
    "        action = np.random.randint(0, n_actions)\n",
    "\n",
    "    return torch.tensor([[action]],device=device)\n",
    "\n",
    "def select_greedy_action(state):\n",
    "    global steps_done\n",
    "    steps_done += 1\n",
    "    policy_net.net.eval()\n",
    "    state_np = state.numpy()\n",
    "\n",
    "    state_temp = np.array(state_np)\n",
    "    state_np = np.repeat(state_temp, BATCH_SIZE, axis=0)\n",
    "\n",
    "    ma, _ = policy_net.net(state_np)\n",
    "    ma = ma.reshape(BATCH_SIZE, policy_net.n_actions*2)[0]\n",
    "    action_mean = ma[::2]\n",
    "\n",
    "    action = np.argmax(action_mean, axis=0)\n",
    "\n",
    "    return torch.tensor([[action]],device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_state(state):\n",
    "    if state == None:\n",
    "        return None\n",
    "    else:\n",
    "        return state / 15\n",
    "\n",
    "def denormalize_state(state):\n",
    "    if state == None:\n",
    "        return None\n",
    "    else:\n",
    "        return state * 15\n",
    "\n",
    "def normalize_reward(reward):\n",
    "    return reward / 6\n",
    "\n",
    "def denormalize_reward(reward):\n",
    "    return reward * 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    policy_net.net.train()\n",
    "    target_net.net.eval()\n",
    "\n",
    "    final_mask = torch.tensor(tuple(map(lambda s: s is None,\n",
    "                                batch.next_state)), device=device, dtype=torch.bool)\n",
    "\n",
    "    state_mean_batch = torch.cat(batch.state).numpy()\n",
    "    action_batch = torch.cat(batch.action).numpy()\n",
    "    reward_batch = torch.cat(batch.reward).numpy()\n",
    "    # next_state_mean_batch = torch.cat([s if s is not None\n",
    "    #                                     else torch.tensor([100.0])\n",
    "    #                                     for s in batch.next_state]).numpy()\n",
    "\n",
    "    next_state_mean_batch = torch.cat([s if s is not None\n",
    "                                        else torch.tensor([100])\n",
    "                                        for s in batch.next_state]).numpy()\n",
    "\n",
    "    # Add uncertainty to the state\n",
    "    state_batch = {'mu': state_mean_batch, 'var': np.zeros_like(state_mean_batch)}\n",
    "    next_state_batch = {'mu': next_state_mean_batch, 'var': np.zeros_like(next_state_mean_batch)}\n",
    "\n",
    "    # Get the next state values from target net\n",
    "    next_state_values_mu_f, next_state_values_var_f = target_net.net(next_state_batch['mu'])\n",
    "\n",
    "    # Reshape to 2D\n",
    "    next_state_values_mu_f = next_state_values_mu_f.reshape(BATCH_SIZE, target_net.n_actions*2)\n",
    "    next_state_values_var_f = next_state_values_var_f.reshape(BATCH_SIZE, target_net.n_actions*2)\n",
    "\n",
    "    # Along the first axis, select the first and the third columns of the 2D array next_state_values_mu\n",
    "    next_state_values_mu = next_state_values_mu_f[:, [0, 2, 4, 6]]\n",
    "    next_state_values_var = next_state_values_var_f[:, [0, 2, 4, 6]] + next_state_values_mu_f[:, [1, 3, 5, 7]]\n",
    "\n",
    "    next_state_values_samples = np.zeros((BATCH_SIZE, target_net.n_actions))\n",
    "    for i in range(BATCH_SIZE):\n",
    "        for j in range(target_net.n_actions):\n",
    "            next_state_values_samples[i, j] = np.random.normal(next_state_values_mu[i, j], np.sqrt(next_state_values_var[i, j]))\n",
    "\n",
    "    # Keep the maximum next state value according to the samples\n",
    "    max_indices = np.argmax(next_state_values_samples, axis=1)\n",
    "    next_state_values_mu = next_state_values_mu[np.arange(BATCH_SIZE), max_indices]\n",
    "    next_state_values_var = next_state_values_var[np.arange(BATCH_SIZE), max_indices]\n",
    "\n",
    "    # Set the next state values of final states to 0 if next state is final\n",
    "    next_state_values_mu_tensor = torch.tensor(next_state_values_mu, device=device)\n",
    "    next_state_values_var_tensor = torch.tensor(next_state_values_var, device=device)\n",
    "    next_state_values_mu_tensor[final_mask] = 0.0\n",
    "    next_state_values_var_tensor[final_mask] = 1e-4\n",
    "    next_state_values_mu = next_state_values_mu_tensor.numpy()\n",
    "    next_state_values_var = next_state_values_var_tensor.numpy()\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_values_mu = np.array((next_state_values_mu * GAMMA) + reward_batch)\n",
    "    expected_state_values_var = np.array((next_state_values_var * GAMMA**2))\n",
    "\n",
    "    # Infer the policy network using the expected Q values\n",
    "    expected_state_action_values_mu_f, expected_state_action_values_var_f = policy_net.net(state_batch['mu'])\n",
    "\n",
    "    # # Only change the expected Q values where actions are taken\n",
    "    expected_state_action_values_mu_f = expected_state_action_values_mu_f.reshape(BATCH_SIZE, policy_net.n_actions*2)\n",
    "    expected_state_action_values_var_f = expected_state_action_values_var_f.reshape(BATCH_SIZE, policy_net.n_actions*2)\n",
    "    expected_state_action_values_mu = expected_state_action_values_mu_f[:, [0,2,4,6]]\n",
    "    expected_state_action_values_var = expected_state_action_values_var_f[:, [0,2,4,6]] + expected_state_action_values_mu_f[:, [1,3,5,7]]\n",
    "    expected_state_action_values_mu[np.arange(BATCH_SIZE), action_batch.flatten()] = expected_state_values_mu\n",
    "    expected_state_action_values_var[np.arange(BATCH_SIZE), action_batch.flatten()] = expected_state_values_var\n",
    "    # expected_state_action_values_mu[np.arange(self.batchsize), 1-action_batch.flatten()] = np.nan\n",
    "    expected_state_action_values_var[np.arange(BATCH_SIZE), 1-action_batch.flatten()] = 100\n",
    "\n",
    "    expected_state_action_values_mu = expected_state_action_values_mu.flatten()\n",
    "    expected_state_action_values_var = expected_state_action_values_var.flatten()\n",
    "\n",
    "    # Update output layer\n",
    "    out_updater = OutputUpdater(policy_net.net.device)\n",
    "    out_updater.update_heteros(\n",
    "        output_states = policy_net.net.output_z_buffer,\n",
    "        mu_obs = expected_state_action_values_mu,\n",
    "        var_obs = expected_state_action_values_var,\n",
    "        delta_states = policy_net.net.input_delta_z_buffer,\n",
    "    )\n",
    "\n",
    "    # Feed backward\n",
    "    policy_net.net.backward()\n",
    "    policy_net.net.step()\n",
    "\n",
    "    # For numerical stability: clip the variance of the parameters to 1e-8\n",
    "    policy_net_param_temp = policy_net.net.get_state_dict()\n",
    "    for key in policy_net_param_temp:\n",
    "        policy_net_param_temp[key]['var_w']=np.clip(policy_net_param_temp[key]['var_w'], 1e-8, None).tolist()\n",
    "        policy_net_param_temp[key]['var_b']=np.clip(policy_net_param_temp[key]['var_b'], 1e-8, None).tolist()\n",
    "        # Clip the policy_net_param_temp[key]['mu_w'] at 1e8 if it is possitive and -1e8 if it is negative\n",
    "        # policy_net_param_temp[key]['mu_w']=np.sign(policy_net_param_temp[key]['mu_w'])*np.clip(np.abs(policy_net_param_temp[key]['mu_w']), 1e-8, None).tolist()\n",
    "        # policy_net_param_temp[key]['mu_b']=np.sign(policy_net_param_temp[key]['mu_b'])*np.clip(np.abs(policy_net_param_temp[key]['mu_b']), 1e-8, None).tolist()\n",
    "    policy_net.net.load_state_dict(policy_net_param_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract current policy\n",
    "def extract_policy(num_states, episode_i):\n",
    "    policy = np.zeros(num_states)\n",
    "    for state in range(num_states):\n",
    "        if state == 15:  # Terminal state\n",
    "            policy[state] = 10\n",
    "            continue\n",
    "        norm_state = normalize_state(state)\n",
    "        state_tensor = torch.tensor(norm_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        suggested_action = select_greedy_action(state_tensor)\n",
    "        policy[state] = suggested_action\n",
    "\n",
    "    policy_grid = np.array(policy).reshape((grid_size, grid_size))\n",
    "    # Print title of the plot\n",
    "    print(f\"Episode {episode_i}'s policy\")\n",
    "    print(policy_grid)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [11:53<00:00, 14.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    num_episodes = 10000\n",
    "else:\n",
    "    num_episodes = 10000\n",
    "\n",
    "from tqdm import tqdm\n",
    "for i_episode in tqdm(range(num_episodes)):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = normalize_state(state)\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        observation = normalize_state(observation)\n",
    "        reward = normalize_reward(reward)\n",
    "\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        infer_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.net.get_state_dict()\n",
    "        policy_net_state_dict = policy_net.net.get_state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            for key2 in policy_net_state_dict[key]:\n",
    "                target_net_state_dict[key][key2] = (np.asarray(policy_net_state_dict[key][key2])*TAU +\n",
    "                                                    np.asarray(target_net_state_dict[key][key2])*(1-TAU)).tolist()\n",
    "        target_net.net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Extract policy\n",
    "    # extract_policy(num_states, i_episode)\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9999's policy\n",
      "[[ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.]\n",
      " [ 3.  3.  3. 10.]]\n"
     ]
    }
   ],
   "source": [
    "# Extract policy\n",
    "extract_policy(num_states, i_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State values:\n",
      "[[-5.97 -5.75 -5.53 -5.25]\n",
      " [-4.96 -4.7  -4.44 -4.2 ]\n",
      " [-3.96 -3.58 -3.27 -3.23]\n",
      " [-2.98 -1.98 -0.99  0.  ]]\n",
      "\n",
      "State values variance:\n",
      "[[0.20820084 0.20909106 0.21261728 0.21058303]\n",
      " [0.20425035 0.19099228 0.17217425 0.15483041]\n",
      " [0.12794092 0.0903518  0.06258753 0.03582984]\n",
      " [0.04810521 0.09226154 0.19966819 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "values = np.zeros(num_states)\n",
    "values_var = np.zeros(num_states)\n",
    "policy_net.net.eval()\n",
    "for state in range(num_states):\n",
    "    if state == 15:  # Terminal state\n",
    "        continue\n",
    "    norm_state = normalize_state(state)\n",
    "    state_tensor = torch.tensor(norm_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    suggested_action = select_greedy_action(state_tensor)\n",
    "\n",
    "    state_np = state_tensor.numpy()\n",
    "\n",
    "    state_temp = np.array(state_np)\n",
    "    state_np = np.repeat(state_temp, BATCH_SIZE, axis=0)\n",
    "\n",
    "    mQ, sQ = policy_net.net(state_np)\n",
    "    mQ = mQ.reshape(BATCH_SIZE, policy_net.n_actions*2)[0]\n",
    "    sQ = sQ.reshape(BATCH_SIZE, policy_net.n_actions*2)[0]\n",
    "    Q_mean = mQ[::2]\n",
    "    Q_var = sQ[::2] + mQ[1::2]\n",
    "\n",
    "    values[state] = denormalize_reward(Q_mean[suggested_action])\n",
    "    values_var[state] = Q_var[suggested_action] * 6**2\n",
    "\n",
    "value_grid = np.array(values).reshape((grid_size, grid_size))\n",
    "value_var_grid = np.sqrt(np.array(values_var)).reshape((grid_size, grid_size))\n",
    "value_grid = np.round(value_grid, 2)\n",
    "# value_var_grid = np.round(value_var_grid, 2)\n",
    "print(\"\\nState values:\")\n",
    "print(value_grid)\n",
    "print(\"\\nState values variance:\")\n",
    "print(value_var_grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kalman_bar_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
